diff --git a/Makefile b/Makefile
index 759e68a02..67963f845 100644
--- a/Makefile
+++ b/Makefile
@@ -1115,7 +1115,7 @@ export MODORDER := $(extmod_prefix)modules.order
 export MODULES_NSDEPS := $(extmod_prefix)modules.nsdeps
 
 ifeq ($(KBUILD_EXTMOD),)
-core-y		+= kernel/ certs/ mm/ fs/ ipc/ security/ crypto/ block/
+core-y		+= kernel/ certs/ mm/ fs/ ipc/ security/ crypto/ block/ my_syscall/
 
 vmlinux-dirs	:= $(patsubst %/,%,$(filter %/, \
 		     $(core-y) $(core-m) $(drivers-y) $(drivers-m) \
diff --git a/arch/x86/entry/syscalls/syscall_64.tbl b/arch/x86/entry/syscalls/syscall_64.tbl
index 18b5500ea..19a390fa9 100644
--- a/arch/x86/entry/syscalls/syscall_64.tbl
+++ b/arch/x86/entry/syscalls/syscall_64.tbl
@@ -413,5 +413,12 @@
 545	x32	execveat		compat_sys_execveat
 546	x32	preadv2			compat_sys_preadv64v2
 547	x32	pwritev2		compat_sys_pwritev64v2
+
+
+# my system calls
+548	64	my_syscall		sys_my_syscall
+549	64	prime_check_syscall	sys_prime_check_syscall
+550 64  mem_statistic sys_mem_statistic
+551 64  reset_stats   sys_reset_stats
 # This is the end of the legacy x32 range.  Numbers 548 and above are
 # not special and are not to be used for x32-specific syscalls.
diff --git a/include/linux/memory_statistic.h b/include/linux/memory_statistic.h
new file mode 100644
index 000000000..d69ad2803
--- /dev/null
+++ b/include/linux/memory_statistic.h
@@ -0,0 +1,27 @@
+struct memory_statistic {
+  // 1. The current number of pages in the current LRU page lists
+  // ANON: ACTIVE, INACTIVE
+  // FILE: ACTIVE, INACTIVE
+  unsigned long anon_pages;                      
+  unsigned long file_pages;   
+
+  // 2. The current number of pages those reference bits are set
+  // in active/inactive list
+  unsigned long active_list_ref_pages;           
+  unsigned long inactive_list_ref_pages; 
+  
+  // 3. The cumulative number of pages 
+  // moved from active list to inactive list and vice versa
+  unsigned long total_active_to_inactive_pages; 
+  unsigned long total_inactive_to_active_pages;
+
+  // 4. The cumulative number of pages evicted from inactive list
+  // When we reboot the system, reset the cumulative numbers
+  unsigned long total_taken_from_inactive_pages;
+};
+
+extern unsigned long my_nr_active_to_inactive;
+extern unsigned long my_nr_inactive_to_active;
+extern unsigned long my_nr_evicted;
+
+extern int COUNTER_BASED_CLOCK;
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 7f8ee09c7..8c8f76600 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -234,6 +234,8 @@ struct page {
 					   not kmapped, ie. highmem) */
 #endif /* WANT_PAGE_VIRTUAL */
 
+  unsigned long reference_counter;
+
 #ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS
 	int _last_cpupid;
 #endif
diff --git a/include/linux/syscalls.h b/include/linux/syscalls.h
index 252243c77..65e2ebec1 100644
--- a/include/linux/syscalls.h
+++ b/include/linux/syscalls.h
@@ -72,6 +72,8 @@ struct mount_attr;
 struct landlock_ruleset_attr;
 enum landlock_rule_type;
 
+struct memory_statistic;
+
 #include <linux/types.h>
 #include <linux/aio_abi.h>
 #include <linux/capability.h>
@@ -85,7 +87,7 @@ enum landlock_rule_type;
 #include <linux/key.h>
 #include <linux/personality.h>
 #include <trace/syscall.h>
-
+#include <linux/memory_statistic.h>
 #ifdef CONFIG_ARCH_HAS_SYSCALL_WRAPPER
 /*
  * It may be useful for an architecture to override the definitions of the
@@ -1272,6 +1274,13 @@ asmlinkage long sys_old_mmap(struct mmap_arg_struct __user *arg);
  */
 asmlinkage long sys_ni_syscall(void);
 
+asmlinkage long sys_my_syscall(void);
+
+asmlinkage long sys_prime_check_syscall(void);
+
+asmlinkage long sys_mem_statistic(struct memory_statistic* stats);
+
+asmlinkage long sys_reset_stats(void);
 #endif /* CONFIG_ARCH_HAS_SYSCALL_WRAPPER */
 
 
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 23d3339ac..378258c35 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -1558,12 +1558,14 @@ static void free_one_page(struct zone *zone,
 	spin_unlock_irqrestore(&zone->lock, flags);
 }
 
+
 static void __meminit __init_single_page(struct page *page, unsigned long pfn,
 				unsigned long zone, int nid)
 {
 	mm_zero_struct_page(page);
 	set_page_links(page, zone, nid, pfn);
 	init_page_count(page);
+  page->reference_counter =0;
 	page_mapcount_reset(page);
 	page_cpupid_reset_last(page);
 	page_kasan_tag_reset(page);
diff --git a/mm/swap.c b/mm/swap.c
index af3cad4e5..cbbee9601 100644
--- a/mm/swap.c
+++ b/mm/swap.c
@@ -43,6 +43,9 @@
 #define CREATE_TRACE_POINTS
 #include <trace/events/pagemap.h>
 
+#include <linux/memory_statistic.h>
+extern unsigned long my_nr_inactive_to_active;
+
 /* How many pages do we try to swap or page in/out together? */
 int page_cluster;
 
@@ -306,6 +309,7 @@ static void __activate_page(struct page *page, struct lruvec *lruvec)
 		trace_mm_lru_activate(page);
 
 		__count_vm_events(PGACTIVATE, nr_pages);
+		my_nr_inactive_to_active+=nr_pages;
 		__count_memcg_events(lruvec_memcg(lruvec), PGACTIVATE,
 				     nr_pages);
 	}
diff --git a/mm/vmscan.c b/mm/vmscan.c
index 74296c2d1..af0d63e66 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -59,9 +59,18 @@
 
 #include "internal.h"
 
+#include <linux/memory_statistic.h>
+#include <linux/mmzone.h>
 #define CREATE_TRACE_POINTS
 #include <trace/events/vmscan.h>
 
+
+extern unsigned long my_nr_active_to_inactive=0;
+extern unsigned long my_nr_inactive_to_active=0;
+extern unsigned long my_nr_evicted=0;
+
+extern int COUNTER_BASED_CLOCK=1;
+
 struct scan_control {
 	/* How many pages shrink_list() should reclaim */
 	unsigned long nr_to_reclaim;
@@ -1373,7 +1382,7 @@ static unsigned int shrink_page_list(struct list_head *page_list,
 	unsigned int nr_reclaimed = 0;
 	unsigned int pgactivate = 0;
 	bool do_demote_pass;
-
+	int temp_nr_demote;
 	memset(stat, 0, sizeof(*stat));
 	cond_resched();
 	do_demote_pass = can_demote(pgdat->node_id, sc);
@@ -1754,7 +1763,7 @@ static unsigned int shrink_page_list(struct list_head *page_list,
 		 * all base pages.
 		 */
 		nr_reclaimed += nr_pages;
-
+		//
 		/*
 		 * Is there need to periodically free_page_list? It would
 		 * appear not as the counts should be low
@@ -1763,6 +1772,7 @@ static unsigned int shrink_page_list(struct list_head *page_list,
 			destroy_compound_page(page);
 		else
 			list_add(&page->lru, &free_pages);
+		
 		continue;
 
 activate_locked_split:
@@ -1795,7 +1805,9 @@ static unsigned int shrink_page_list(struct list_head *page_list,
 	/* 'page_list' is always empty here */
 
 	/* Migrate pages selected for demotion */
-	nr_reclaimed += demote_page_list(&demote_pages, pgdat);
+	temp_nr_demote = demote_page_list(&demote_pages, pgdat);
+	nr_reclaimed += temp_nr_demote;
+
 	/* Pages that could not be demoted are still in @demote_pages */
 	if (!list_empty(&demote_pages)) {
 		/* Pages which failed to demoted go back on @page_list for retry: */
@@ -1811,6 +1823,7 @@ static unsigned int shrink_page_list(struct list_head *page_list,
 	free_unref_page_list(&free_pages);
 
 	list_splice(&ret_pages, page_list);
+	my_nr_inactive_to_active += pgactivate;
 	count_vm_events(PGACTIVATE, pgactivate);
 
 	return nr_reclaimed;
@@ -2266,6 +2279,8 @@ shrink_inactive_list(unsigned long nr_to_scan, struct lruvec *lruvec,
 	nr_taken = isolate_lru_pages(nr_to_scan, lruvec, &page_list,
 				     &nr_scanned, sc, lru);
 
+
+
 	__mod_node_page_state(pgdat, NR_ISOLATED_ANON + file, nr_taken);
 	item = current_is_kswapd() ? PGSCAN_KSWAPD : PGSCAN_DIRECT;
 	if (!cgroup_reclaim(sc))
@@ -2285,10 +2300,13 @@ shrink_inactive_list(unsigned long nr_to_scan, struct lruvec *lruvec,
 
 	__mod_node_page_state(pgdat, NR_ISOLATED_ANON + file, -nr_taken);
 	item = current_is_kswapd() ? PGSTEAL_KSWAPD : PGSTEAL_DIRECT;
-	if (!cgroup_reclaim(sc))
+	if (!cgroup_reclaim(sc)){
+		my_nr_evicted+=nr_reclaimed;
 		__count_vm_events(item, nr_reclaimed);
+	}
 	__count_memcg_events(lruvec_memcg(lruvec), item, nr_reclaimed);
 	__count_vm_events(PGSTEAL_ANON + file, nr_reclaimed);
+	
 	spin_unlock_irq(&lruvec->lru_lock);
 
 	lru_note_cost(lruvec, file, stat.nr_pageout);
@@ -2320,6 +2338,8 @@ shrink_inactive_list(unsigned long nr_to_scan, struct lruvec *lruvec,
 
 	trace_mm_vmscan_lru_shrink_inactive(pgdat->node_id,
 			nr_scanned, nr_reclaimed, &stat, sc->priority, file);
+	
+
 	return nr_reclaimed;
 }
 
@@ -2345,6 +2365,8 @@ static void shrink_active_list(unsigned long nr_to_scan,
 			       struct scan_control *sc,
 			       enum lru_list lru)
 {
+	//int COUNTER_BASED_CLOCK; // active counter based clock second chance algorithm
+
 	unsigned long nr_taken;
 	unsigned long nr_scanned;
 	unsigned long vm_flags;
@@ -2357,6 +2379,7 @@ static void shrink_active_list(unsigned long nr_to_scan,
 	int file = is_file_lru(lru);
 	struct pglist_data *pgdat = lruvec_pgdat(lruvec);
 
+	//COUNTER_BASED_CLOCK = 1;
 	lru_add_drain();
 
 	spin_lock_irq(&lruvec->lru_lock);
@@ -2373,15 +2396,16 @@ static void shrink_active_list(unsigned long nr_to_scan,
 	spin_unlock_irq(&lruvec->lru_lock);
 
 	while (!list_empty(&l_hold)) {
-		cond_resched();
+		cond_resched(); // no timer interrupt -> no overhead, just wait rescheduling periodically
 		page = lru_to_page(&l_hold);
 		list_del(&page->lru);
 
+		// if page is unevictable -> put back lru list
 		if (unlikely(!page_evictable(page))) {
 			putback_lru_page(page);
 			continue;
 		}
-
+		// if buffer heads over limit ->
 		if (unlikely(buffer_heads_over_limit)) {
 			if (page_has_private(page) && trylock_page(page)) {
 				if (page_has_private(page))
@@ -2390,27 +2414,57 @@ static void shrink_active_list(unsigned long nr_to_scan,
 			}
 		}
 
+		if(COUNTER_BASED_CLOCK){
+			// add reference bit to the reference counter 
+			// max value + 1 -> overflow -> just clear reference bit
+			if(page->reference_counter + PageReferenced(page) > 0){
+				// add count of visit this page
+				// page_ref_add(page, TestClearPageReferenced(page));
+				page->reference_counter += TestClearPageReferenced(page);
+			}else{
+				ClearPageReferenced(page);
+			}
+		}
+
+		// if positive, page was accessed recently ??
 		if (page_referenced(page, 0, sc->target_mem_cgroup,
-				    &vm_flags)) {
+						&vm_flags)) {
+							
 			/*
-			 * Identify referenced, file-backed active pages and
-			 * give them one more trip around the active list. So
-			 * that executable code get better chances to stay in
-			 * memory under moderate memory pressure.  Anon pages
-			 * are not likely to be evicted by use-once streaming
-			 * IO, plus JVM can create lots of anon VM_EXEC pages,
-			 * so we ignore them here.
-			 */
+			* Identify referenced, file-backed active pages and
+			* give them one more trip around the active list. So
+			* that executable code get better chances to stay in
+			* memory under moderate memory pressure.  Anon pages
+			* are not likely to be evicted by use-once streaming
+			* IO, plus JVM can create lots of anon VM_EXEC pages,
+			* so we ignore them here.
+			*/
 			if ((vm_flags & VM_EXEC) && page_is_file_lru(page)) {
 				nr_rotated += thp_nr_pages(page);
+				if(COUNTER_BASED_CLOCK){
+						page->reference_counter--;
+				}
 				list_add(&page->lru, &l_active);
 				continue;
 			}
 		}
 
-		ClearPageActive(page);	/* we are de-activating */
-		SetPageWorkingset(page);
-		list_add(&page->lru, &l_inactive);
+		if(COUNTER_BASED_CLOCK==1){
+			if(page->reference_counter == 0){
+				ClearPageActive(page);	/* we are de-activating */
+				SetPageWorkingset(page);
+				list_add(&page->lru, &l_inactive);
+			}
+			else{
+				page->reference_counter--;
+				list_add(&page->lru, &l_active);
+			}
+		}else{
+			ClearPageActive(page);	/* we are de-activating */
+			SetPageWorkingset(page);
+			list_add(&page->lru, &l_inactive);
+		}
+		
 	}
 
 	/*
@@ -2420,6 +2474,8 @@ static void shrink_active_list(unsigned long nr_to_scan,
 
 	nr_activate = move_pages_to_lru(lruvec, &l_active);
 	nr_deactivate = move_pages_to_lru(lruvec, &l_inactive);
+	my_nr_active_to_inactive+=nr_deactivate;
+
 	/* Keep all free pages in l_active list */
 	list_splice(&l_inactive, &l_active);
 
@@ -2427,6 +2483,7 @@ static void shrink_active_list(unsigned long nr_to_scan,
 	__count_memcg_events(lruvec_memcg(lruvec), PGDEACTIVATE, nr_deactivate);
 
 	__mod_node_page_state(pgdat, NR_ISOLATED_ANON + file, -nr_taken);
+	
 	spin_unlock_irq(&lruvec->lru_lock);
 
 	mem_cgroup_uncharge_list(&l_active);
@@ -2887,7 +2944,7 @@ static void shrink_lruvec(struct lruvec *lruvec, struct scan_control *sc)
 	}
 	blk_finish_plug(&plug);
 	sc->nr_reclaimed += nr_reclaimed;
-
+	
 	/*
 	 * Even if we did not try to evict anon pages at all, we want to
 	 * rebalance the anon lru active/inactive ratio.
@@ -3432,7 +3489,7 @@ static unsigned long do_try_to_free_pages(struct zonelist *zonelist,
 
 	if (sc->nr_reclaimed)
 		return sc->nr_reclaimed;
-
+	
 	/* Aborted reclaim to try compaction? don't OOM, then */
 	if (sc->compaction_ready)
 		return 1;
diff --git a/my_syscall/Makefile b/my_syscall/Makefile
new file mode 100644
index 000000000..93a936e54
--- /dev/null
+++ b/my_syscall/Makefile
@@ -0,0 +1,4 @@
+obj-y += memory_statistics.o
+obj-y += reset_stats.o
+obj-y += my_func.o
+obj-y += prime_check.o
\ No newline at end of file
diff --git a/my_syscall/memory_statistics.c b/my_syscall/memory_statistics.c
new file mode 100644
index 000000000..5f076fd53
--- /dev/null
+++ b/my_syscall/memory_statistics.c
@@ -0,0 +1,90 @@
+#include <linux/kernel.h>
+#include <linux/syscalls.h>
+#include <linux/uaccess.h>
+#include <linux/mmzone.h>
+#include <linux/mm.h>
+#include <linux/memcontrol.h>
+#include <linux/list.h>
+#include <linux/numa.h>
+#include <linux/mm_inline.h>
+#include <linux/cgroup.h>
+#include <linux/page-flags.h>
+#include <linux/vmstat.h>
+
+extern unsigned long my_nr_active_to_inactive;
+extern unsigned long my_nr_inactive_to_active;
+extern unsigned long my_nr_evicted;
+
+void init_kernel_stats(struct memory_statistic *kernel_stats);
+
+SYSCALL_DEFINE1(mem_statistic,  struct memory_statistic *, stats)
+{
+  int nid;
+  struct page *page;
+  struct list_head *active_page, *inactive_page;
+
+  struct mem_cgroup *memcg=NULL;
+  struct pglist_data *current_pglist = NULL;
+  struct lruvec *lruvec = NULL;
+
+  struct memory_statistic *kernel_stats = kmalloc(sizeof(*kernel_stats), GFP_KERNEL);
+
+  init_kernel_stats(kernel_stats);
+
+  kernel_stats->total_active_to_inactive_pages = my_nr_active_to_inactive;
+  kernel_stats->total_inactive_to_active_pages = my_nr_inactive_to_active;
+  kernel_stats->total_taken_from_inactive_pages = my_nr_evicted;
+  kernel_stats->anon_pages = global_node_page_state(NR_ACTIVE_ANON)+global_node_page_state(NR_INACTIVE_ANON);
+  kernel_stats->file_pages =  global_node_page_state(NR_ACTIVE_FILE)+global_node_page_state(NR_INACTIVE_FILE);
+ 
+  for(nid=0;nid<MAX_NUMNODES; nid++){
+    if(NODE_DATA(nid) == NULL) continue;
+    current_pglist = NODE_DATA(nid);
+
+    memcg = get_mem_cgroup_from_mm(NULL);
+  
+    lruvec = mem_cgroup_lruvec(memcg, current_pglist);
+
+    spin_lock_irq(&lruvec->lru_lock);
+    list_for_each(active_page, &(lruvec->lists[LRU_ACTIVE_ANON])){
+        page = list_entry(active_page, struct page, lru);
+        kernel_stats->active_list_ref_pages  += test_bit(PG_referenced, &page->flags);
+    }
+
+    list_for_each(active_page, &(lruvec->lists[LRU_ACTIVE_FILE])){
+        page = list_entry(active_page, struct page, lru);
+        kernel_stats->active_list_ref_pages  += test_bit(PG_referenced, &page->flags);
+    }
+
+    list_for_each(inactive_page, &(lruvec->lists[LRU_INACTIVE_ANON])){
+        page = list_entry(inactive_page, struct page, lru);
+         kernel_stats->inactive_list_ref_pages  += test_bit(PG_referenced, &page->flags);
+    }
+
+    list_for_each(inactive_page, &(lruvec->lists[LRU_INACTIVE_FILE])){
+        page = list_entry(inactive_page, struct page, lru);
+         kernel_stats->inactive_list_ref_pages  += test_bit(PG_referenced, &page->flags);
+    }
+    spin_unlock_irq(&lruvec->lru_lock);
+
+  }
+
+  // Copy kernel-space memstats into user-space memstats
+  if(copy_to_user(stats, kernel_stats, sizeof(*kernel_stats))) {
+    return -EFAULT;
+  }
+
+  kfree(kernel_stats);
+  return 0;
+}
+
+void init_kernel_stats(struct memory_statistic *kernel_stats) {
+  kernel_stats->anon_pages = 0;
+  kernel_stats->file_pages = 0;
+  kernel_stats->active_list_ref_pages = 0;
+  kernel_stats->inactive_list_ref_pages = 0;
+  kernel_stats->total_active_to_inactive_pages = 0;
+  kernel_stats->total_inactive_to_active_pages = 0;
+  kernel_stats->total_taken_from_inactive_pages = 0;
+}
+
diff --git a/my_syscall/my_func.c b/my_syscall/my_func.c
new file mode 100644
index 000000000..d93fbc100
--- /dev/null
+++ b/my_syscall/my_func.c
@@ -0,0 +1,9 @@
+/* Start of my_func.c */
+#include <linux/kernel.h>
+#include <linux/syscalls.h>
+
+SYSCALL_DEFINE0(my_syscall)
+{
+	printk(KERN_INFO "This is my first system call!\n");
+	return 0;
+}
diff --git a/my_syscall/prime_check.c b/my_syscall/prime_check.c
new file mode 100644
index 000000000..444598075
--- /dev/null
+++ b/my_syscall/prime_check.c
@@ -0,0 +1,52 @@
+#include <linux/kernel.h>
+#include <linux/syscalls.h>
+#include <linux/uaccess.h>
+/*
+SYSCALL_DEFINE1(prime_check_syscall, int*, number_buf)
+{
+	
+	int i, number;
+	copy_from_user(&number, number_buf, sizeof(int));
+	
+	if(number<=1) return 0;
+	for(i=2;i<=number/2;i++){
+		if(number%i==0) return 0;
+	}
+	return 1;
+}
+*/
+
+SYSCALL_DEFINE3(prime_check_syscall, 
+		int**, buf,
+		int*, len_buf,
+		int*, count_buf)
+{
+	int* numbers;
+	int i,j, count=0;
+	int isPrime;
+	int len;
+//	int number;
+	if(copy_from_user(&len, len_buf, sizeof(len))) return -EFAULT;
+	if(copy_from_user(&numbers, buf, sizeof(numbers))) return -EFAULT;
+
+	
+	//printk(KERN_INFO "Number is :%d\n", number);
+	
+	for(i=0;i<len;i++){
+		isPrime = 1;
+
+		if(numbers[i] <=1) continue;
+		for(j=2;j<=numbers[i]/2;j++){
+			if(numbers[i] % j == 0){
+			       	isPrime=0;
+				break;
+			}
+		}
+		if(isPrime == 1) count++;
+
+	}
+
+	copy_to_user(count_buf, &count, sizeof(count));
+	return 0;
+}
+
diff --git a/my_syscall/reset_stats.c b/my_syscall/reset_stats.c
new file mode 100644
index 000000000..002694a78
--- /dev/null
+++ b/my_syscall/reset_stats.c
@@ -0,0 +1,27 @@
+#include <linux/kernel.h>
+#include <linux/syscalls.h>
+#include <linux/uaccess.h>
+#include <linux/sched.h>
+
+/*
+struct memory_statistic {
+  unsigned long free_pages;                                          
+  unsigned long active_list_pages;               
+  unsigned long inactive_list_pages;             
+  unsigned long active_list_ref_pages;           
+  unsigned long inactive_list_ref_pages;         
+  unsigned long total_active_to_inactive_pages;  
+  unsigned long total_taken_from_inactive_pages;
+};
+*/
+extern unsigned long my_nr_active_to_inactive;
+extern unsigned long my_nr_inactive_to_active;
+
+SYSCALL_DEFINE0(reset_stats)
+{
+  my_nr_active_to_inactive = 0;
+  my_nr_inactive_to_active = 0;
+
+  return 0;
+}
+
